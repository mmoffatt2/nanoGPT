# explorations/sharing.yaml
# Sweep: size-sharing × seq-sharing × symmetry for both MLP & Attn
#
# Each key with a *list* value is expanded cartesian-product style
# by the run_experiments.py launcher.

# ── dataset & compute ──────────────────────────────────────────────
dataset: ["minipile"]
device:  ["cuda"]                    # or ["cpu"] if you like
compile: [true]                      # tiny net ⇒ compile just works
never_save_checkpoint: [true]
dtype: ["bfloat16"]       # try both cheap dtypes

# ── base model hyper-params (SMALL net) ────────────────────────────
max_iters: [10000]
eval_interval: [10000]
eta_variant: ["iteration"]

# ── SHARING GRID  (cartesian product) ──────────────────────────────
n_layer: [8]
n_embd: [512]
n_head: [8]
block_size: [1024]
batch_size: [64]

# Size-sharing: reuse block every k layers
shared_attn_size: [1, 2, 3]          # 1 = no size-sharing
shared_mlp_size:  [1, 2, 3]

# Sequence-sharing: cyclic A-B-C pattern length
shared_attn_seq:  [1, 2, 3, 4]          # 1 = disabled
shared_mlp_seq:   [1, 2, 3, 4]

# Symmetry mirror
shared_attn_sym: [false, true]
shared_mlp_sym:  [false, true]

